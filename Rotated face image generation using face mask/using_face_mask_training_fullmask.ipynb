{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_facemask import *\n",
    "from model_facemask import *\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=2e-4\n",
    "batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=['D:/data/train_data_ia.npy',\n",
    "       'D:/data/train_data_ib.npy',\n",
    "       'D:/data/train_big_pose_a.npy',\n",
    "       'D:/data/train_big_pose_b.npy',\n",
    "       'D:/data/train_data_pa_mask_full.npy',\n",
    "       'D:/data/train_data_pb_mask_full.npy',\n",
    "       'D:/data/train_data_gray_b.npy'\n",
    "      ]\n",
    "test=['D:/data/test_data_ia.npy',\n",
    "      'D:/data/test_data_ib.npy',\n",
    "      'D:/data/test_big_pose_a.npy',\n",
    "      'D:/data/test_big_pose_b.npy',\n",
    "      'D:/data/test_data_pa_mask_full.npy',\n",
    "      'D:/data/test_data_pb_mask_full.npy',\n",
    "      'D:/data/test_data_gray_b.npy']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = transforms.Compose([ToTensor()])\n",
    "\n",
    "# train_dataset=ImageDataset(fnames=test,transform=composed)\n",
    "test_dataset=ImageDataset(fnames=train,transform=composed)\n",
    "# train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen=Generator().cuda()\n",
    "gen=UNet_generator().cuda()\n",
    "dis1=Discriminator1().cuda()\n",
    "dis2=Discriminator2().cuda()\n",
    "criterion=nn.MSELoss()\n",
    "gen_optimizer=torch.optim.Adam(gen.parameters(),lr=learning_rate)\n",
    "dis1_optimizer=torch.optim.Adam(dis1.parameters(),lr=learning_rate)\n",
    "dis2_optimizer=torch.optim.Adam(dis2.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ,(8,5, 128, 128),(8,5, 128, 128)\n",
    "# summary(gen,(13,128, 128))\n",
    "# summary(dis1,(6,128, 128))\n",
    "# summary(dis2,(8,128, 128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretrained data using lightCNN 29layers\n",
    "pretrained_dict=torch.load('LightCNN_29Layers_checkpoint.pth.tar')\n",
    "lightcnnmodel=network_29layers(resblock, [1, 2, 3, 4]).cuda()\n",
    "lightcnnmodel_dict = lightcnnmodel.state_dict()\n",
    "# # 1. filter out unnecessary keys\n",
    "pretrained_dict = {k.replace(\"module.\",\"\"): v for k, v in pretrained_dict['state_dict'].items() if k.replace(\"module.\",\"\") in lightcnnmodel_dict}\n",
    "# # 2. overwrite entries in the existing state dict\n",
    "lightcnnmodel_dict.update(pretrained_dict) \n",
    "# # 3. load the new state dict\n",
    "lightcnnmodel.load_state_dict(pretrained_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(lightcnnmodel,(1,128, 128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "def reset_grad():\n",
    "    gen_optimizer.zero_grad()\n",
    "    dis1_optimizer.zero_grad()\n",
    "    dis2_optimizer.zero_grad()\n",
    "def denorm(x):\n",
    "    out = (x+1)/2\n",
    "    return out.clamp(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(learning_rate=2e-4, num_epochs=200):\n",
    "    gen.load_state_dict(torch.load(\"saver/mask/mask_G.ckpt\"))\n",
    "    dis1.load_state_dict(torch.load(\"saver/mask/mask_D1.ckpt\"))\n",
    "    dis2.load_state_dict(torch.load(\"saver/mask/mask_D2.ckpt\"))\n",
    "    #training step\n",
    "    curr_lr = learning_rate\n",
    "    start = time.time()\n",
    "    total_step = len(train_loader)\n",
    "    \n",
    "    for epochs in range(num_epochs):\n",
    "        for i, (ia,ib,pa,pb,pam,pbm,gray_b)in enumerate(train_loader):\n",
    "\n",
    "            \n",
    "            image_a = Variable(ia).cuda()\n",
    "            image_b = Variable(ib).cuda() \n",
    "            pose_a = Variable(pa).cuda()\n",
    "            pose_b = Variable(pb).cuda()\n",
    "            posemask_a = Variable(pam).cuda()\n",
    "            posemask_b = Variable(pbm).cuda()\n",
    "            gray_b = Variable(gray_b).cuda()\n",
    "            i_num=len(image_a)\n",
    "            #real label fake label\n",
    "            real_labels = torch.ones(1, i_num).cuda()\n",
    "            fake_labels = torch.zeros(1,i_num).cuda()\n",
    "\n",
    "            '''\n",
    "            real image 인지 \n",
    "            dis_ii_val1 = D1(ib,ia)\n",
    "            dis_ii_val2 = D1(i^b,ia)\n",
    "\n",
    "            real pose 인지\n",
    "            dis_pe_val1=D2(ib,pb)\n",
    "            dis_pe_val2=D2(i^b,pb)\n",
    "            '''\n",
    "\n",
    "\n",
    "            #discriminator1 학습 --> 이미지의 형태를 판별\n",
    "            gen_image,gen_gray_img= gen.forward(image_a,pose_a,pose_b)      \n",
    "            #얘네를 참으로 학습하게한다\n",
    "            dis_ii_val1=dis1.forward(image_b,image_a)\n",
    "            dis_ii_val2=dis1.forward(gen_image,image_a)\n",
    "            dis_pe_val1=dis2.forward(image_b,pose_b)\n",
    "            dis_pe_val2=dis2.forward(gen_image,pose_b)\n",
    "#             print(dis_ii_val1.shape,real_labels.shape,fake_labels.shape)\n",
    "\n",
    "\n",
    "            #light cnn feature extractor\n",
    "\n",
    "            D_pool_i_gb,D_fc_i_gb=lightcnnmodel.forward(gen_gray_img)\n",
    "            D_pool_i_b,D_fc_i_b=lightcnnmodel.forward(gray_b)\n",
    "\n",
    "            loss_pix_t= multi_scale_pixel_wise_loss(image_b,gen_image)\n",
    "            loss_ii_adv_t=criterion(dis_ii_val1,real_labels)\n",
    "            loss_pe_adv_t=0\n",
    "            loss_ip_t=identity_preserving_loss(D_pool_i_gb,D_pool_i_b,D_fc_i_gb,D_fc_i_b)\n",
    "            loss_tv_t=total_variant_regularization(gen_image)\n",
    "            real_total_loss=total_loss(loss_pix_t,loss_ii_adv_t,loss_pe_adv_t,loss_ip_t,loss_tv_t)\n",
    "            \n",
    "            loss_pix_f= multi_scale_pixel_wise_loss(image_b,gen_image)\n",
    "            loss_ii_adv_f=criterion(dis_ii_val2,fake_labels)\n",
    "            loss_pe_adv_f=0\n",
    "            loss_ip_f=identity_preserving_loss(D_pool_i_gb,D_pool_i_b,D_fc_i_gb,D_fc_i_b)\n",
    "            loss_tv_f=total_variant_regularization(gen_image)\n",
    "            fake_total_loss=total_loss(loss_pix_f,loss_ii_adv_f,loss_pe_adv_f,loss_ip_f,loss_tv_f)\n",
    "            \n",
    "            d1_total_loss=real_total_loss+fake_total_loss\n",
    "            # Backprop and optimize\n",
    "            dis1_optimizer.zero_grad()\n",
    "            d1_total_loss.backward()\n",
    "            dis1_optimizer.step()\n",
    "\n",
    "\n",
    "            #discriminator2 학습 --> pose 판별\n",
    "            gen_image,gen_gray_img= gen.forward(image_a,pose_a,pose_b)    \n",
    "            #얘네를 참으로 학습하게한다\n",
    "\n",
    "            dis_ii_val1=dis1.forward(image_b,image_a)\n",
    "            dis_ii_val2=dis1.forward(gen_image,image_a)\n",
    "            dis_pe_val1=dis2.forward(image_b,pose_b)\n",
    "            dis_pe_val2=dis2.forward(gen_image,pose_b)\n",
    "\n",
    "            \n",
    "            #light cnn feature extractor\n",
    "\n",
    "            D_pool_i_gb,D_fc_i_gb=lightcnnmodel.forward(gen_gray_img)\n",
    "            D_pool_i_b,D_fc_i_b=lightcnnmodel.forward(gray_b)\n",
    "\n",
    "            loss_pix_t = multi_scale_pixel_wise_loss(image_b,gen_image)\n",
    "            loss_ii_adv_t=0\n",
    "            loss_pe_adv_t=criterion(dis_pe_val1,real_labels)\n",
    "            loss_ip_t=identity_preserving_loss(D_pool_i_gb,D_pool_i_b,D_fc_i_gb,D_fc_i_b)\n",
    "            loss_tv_t=total_variant_regularization(gen_image)        \n",
    "            real_total_loss=total_loss(loss_pix_t,loss_ii_adv_t,loss_pe_adv_t,loss_ip_t,loss_tv_t)\n",
    "\n",
    "            loss_pix_f = multi_scale_pixel_wise_loss(image_b,gen_image)\n",
    "            loss_ii_adv_f=0\n",
    "            loss_pe_adv_f=criterion(dis_pe_val2,fake_labels)\n",
    "            loss_ip_f=identity_preserving_loss(D_pool_i_gb,D_pool_i_b,D_fc_i_gb,D_fc_i_b)\n",
    "            loss_tv_f=total_variant_regularization(gen_image)        \n",
    "            fake_total_loss=total_loss(loss_pix_f,loss_ii_adv_f,loss_pe_adv_f,loss_ip_f,loss_tv_f)\n",
    "\n",
    "            d2_total_loss=real_total_loss+fake_total_loss\n",
    "            dis2_optimizer.zero_grad()\n",
    "            d2_total_loss.backward()\n",
    "            dis2_optimizer.step()\n",
    "\n",
    "            #gen_image's shape : width :\n",
    "            gen_image,gen_gray_img= gen.forward(image_a,pose_a,pose_b)     \n",
    "            #얘네를 참으로 학습하게한다\n",
    "            dis_ii_val1=dis1.forward(image_b,image_a)\n",
    "            dis_ii_val2=dis1.forward(gen_image,image_a)\n",
    "            dis_pe_val1=dis2.forward(image_b,pose_b)\n",
    "            dis_pe_val2=dis2.forward(gen_image,pose_b)\n",
    "            posemask_loss=pose_mask_loss(gen_image,image_b,posemask_b)\n",
    "\n",
    "            #generator\n",
    "            \n",
    "            D_pool_i_gb,D_fc_i_gb=lightcnnmodel.forward(gen_gray_img)\n",
    "            D_pool_i_b,D_fc_i_b=lightcnnmodel.forward(gray_b)\n",
    "\n",
    "            loss_pix_t = multi_scale_pixel_wise_loss(image_b,gen_image)\n",
    "            loss_ii_adv_t=criterion(dis_ii_val2,real_labels)\n",
    "            loss_pe_adv_t=criterion(dis_pe_val2,real_labels)\n",
    "            loss_ip_t=identity_preserving_loss(D_pool_i_gb,D_pool_i_b,D_fc_i_gb,D_fc_i_b)\n",
    "            loss_tv_t=total_variant_regularization(gen_image)\n",
    "            g_total_loss=total_loss(loss_pix_t,loss_ii_adv_t,loss_pe_adv_t,loss_ip_t,loss_tv_t)+posemask_loss\n",
    "\n",
    "            # Backprop and optimize\n",
    "            gen_optimizer.zero_grad()\n",
    "            g_total_loss.backward()\n",
    "            gen_optimizer.step()\n",
    "\n",
    "\n",
    "    #         if (i+1) % 10 == 0:\n",
    "        print('Epoch [{}/{}], Step [{}/{}], d1_loss: {:.4f},d2_loss: {:.4f}, g_total_loss: {:.4f}'\n",
    "              .format(epochs, num_epochs, i+1, total_step, d1_total_loss.mean(), d2_total_loss.mean(),\n",
    "                      g_total_loss.mean()))\n",
    "        # Decay learning rate\n",
    "        if (epochs+1) % 20 == 0:\n",
    "            curr_lr /= 3\n",
    "            update_lr(gen_optimizer, curr_lr)\n",
    "            update_lr(dis1_optimizer, curr_lr)\n",
    "            update_lr(dis2_optimizer, curr_lr)\n",
    "        if (epochs+1) % 10 == 0:\n",
    "            torch.save(gen.state_dict(),'saver/mask/mask_G.ckpt')\n",
    "            torch.save(dis1.state_dict(),'saver/mask/mask_D1.ckpt')\n",
    "            torch.save(dis2.state_dict(),'saver/mask/mask_D2.ckpt')\n",
    "\n",
    "        save_image(denorm(gen_image),'samples/mask/gen_images-{}.png'.format(epochs+1))\n",
    "        save_image(denorm(image_a),'samples/mask/a_image-{}.png'.format(epochs+1))\n",
    "        save_image(denorm(image_b),'samples/mask/b_image-{}.png'.format(epochs+1))\n",
    "    finished = time.time()\n",
    "    hours = finished-start\n",
    "\n",
    "    print(\"training finished! %d minutes\" % hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training(learning_rate=2e-4, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fullface mask\n",
    "def testing():\n",
    "    #test\n",
    "    gen.load_state_dict(torch.load(\"saver/fullmask/mask_G.ckpt\"))\n",
    "    dis1.load_state_dict(torch.load(\"saver/fullmask/mask_D1.ckpt\"))\n",
    "    dis2.load_state_dict(torch.load(\"saver/fullmask/mask_D2.ckpt\"))\n",
    "    for i, (ia,ib,pa,pb,pam,pbm,gray_b)in enumerate(test_loader):\n",
    "        image_a = Variable(ia).cuda()\n",
    "        image_b = Variable(ib).cuda() \n",
    "        pose_a = Variable(pa).cuda()\n",
    "        pose_b = Variable(pb).cuda()\n",
    "        gen_image,_= gen.forward(image_a,pose_a,pose_b)\n",
    "        save_image(denorm(image_a),'results/mask1_fullmask/a_image-{}.png'.format(i+1))\n",
    "        save_image(denorm(gen_image),'results/mask1_fullmask/gen_image-{}.png'.format(i+1))\n",
    "        save_image(denorm(image_b),'results/mask1_fullmask/b_image-{}.png'.format(i+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nhs\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "C:\\Users\\nhs\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
